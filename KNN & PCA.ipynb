{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726228cd-845b-4ebc-80d8-82a280f482c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16adb71f-e5c0-43e5-a7a2-31f9998ca60d",
   "metadata": {},
   "source": [
    "# Question 1: What is K-Nearest Neighbors (KNN) and how does it work in classification and regression?\n",
    "\n",
    "KNN is a simple supervised learning algorithm used for classification and regression.\n",
    "\n",
    "- It stores all training data.\n",
    "- To predict a new point, it finds the K closest points (neighbors) using a distance metric (like Euclidean).\n",
    "- For classification, it assigns the most common class among the neighbors.\n",
    "- For regression, it averages the values of the neighbors.\n",
    "\n",
    "Example for classification:  \n",
    "New point’s 3 nearest neighbors have labels A, A, B → predict class A.\n",
    "\n",
    "Example for regression:  \n",
    "Nearest neighbors’ values are 10, 12, 14 → predicted value = (10+12+14)/3 = 12.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8db57-01fd-4235-9e63-ad1b15667583",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a33315f-5e35-410a-b5fb-7e8424bba394",
   "metadata": {},
   "source": [
    "# Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
    "\n",
    "The Curse of Dimensionality refers to problems when data has many features (high dimensions):\n",
    "\n",
    "- Data becomes sparse and distances between points become less meaningful.\n",
    "- KNN relies on distance, so it struggles to find true neighbors in high dimensions.\n",
    "- It requires more data to work well.\n",
    "- Computation cost increases with more features.\n",
    "\n",
    "To improve, use feature selection or dimensionality reduction (like PCA) to reduce dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43e73b-72df-4dea-ada9-db09b425ce82",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25596f19-1372-490a-b44b-c2e94de4d71b",
   "metadata": {},
   "source": [
    "# Question 3: What is Principal Component Analysis (PCA)? How is it different from Feature Selection?\n",
    "\n",
    "Principal Component Analysis (PCA) is a method used to reduce the number of features in a dataset by creating new features (called principal components) that capture the most important information (variance) in the data.\n",
    "\n",
    "PCA is unsupervised, meaning it doesn't use the target labels. It transforms the data into a new coordinate system where the first few components keep most of the original information.\n",
    "\n",
    "Steps in PCA:\n",
    "1. Standardize the data\n",
    "2. Calculate the covariance matrix\n",
    "3. Compute eigenvectors and eigenvalues\n",
    "4. Choose top components\n",
    "5. Project data onto these components\n",
    "\n",
    "PCA is useful when there are many features and you want to reduce noise or make the data simpler for algorithms like KNN.\n",
    "\n",
    "Difference between PCA and Feature Selection:\n",
    "\n",
    "- PCA creates new features as combinations of original ones.\n",
    "- Feature selection keeps some of the original features and removes the rest.\n",
    "- PCA is unsupervised. Feature selection can be supervised (using the target variable).\n",
    "- PCA changes the meaning of features, while feature selection keeps them as they are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9925b0b-d1f1-471b-9eb7-ea116b978fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af2aa3d4-1d2c-4fca-8c18-963fb6f15814",
   "metadata": {},
   "source": [
    "# Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
    "\n",
    "- PCA starts by calculating the covariance matrix of the data to understand how features vary with each other.\n",
    "- Eigenvectors are special vectors that show the directions in which the data varies the most. They define new axes (principal components) for the data.\n",
    "- Eigenvalues are numbers associated with each eigenvector that measure the amount of variance (spread or information) along that direction.\n",
    "- In PCA, we find all eigenvectors and their eigenvalues from the covariance matrix.\n",
    "- The eigenvectors with the largest eigenvalues represent the most important directions where the data has the highest variance.\n",
    "- By choosing the top eigenvectors (principal components), we reduce the data’s dimensionality but keep most of its important information.\n",
    "- This process helps in removing noise and redundant features while preserving the structure of the data.\n",
    "- Without eigenvalues and eigenvectors, PCA would not be able to find the best new axes to represent the data efficiently.\n",
    "\n",
    "In summary, eigenvectors determine the directions of maximum variance, and eigenvalues tell us how important those directions are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b626f0-8c83-4c4a-a499-bafb37070b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1586f751-009c-4d36-9137-75a4fb40af2f",
   "metadata": {},
   "source": [
    "# Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
    "\n",
    "- KNN works by finding the nearest neighbors based on distance between data points.\n",
    "- When there are many features (high dimensions), distances become less meaningful because of the curse of dimensionality.\n",
    "- PCA helps by reducing the number of features while keeping most of the important information.\n",
    "- By applying PCA before KNN, we get a lower-dimensional representation of the data.\n",
    "- This makes distance calculations more reliable and speeds up the computation.\n",
    "- The combination improves KNN’s performance and accuracy on high-dimensional data.\n",
    "- So, PCA reduces noise and redundancy, and KNN uses the transformed data to make better predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b933f85-7698-4b7c-9947-7ff6b35ab55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef66c9bb-f806-429e-9a31-be55c2ccd255",
   "metadata": {},
   "source": [
    "# Dataset:\n",
    "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
    "# Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
    "scaling. Compare model accuracy in both cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2e34d58-d2db-4439-9102-868d081e73d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\shwet\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\shwet\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\shwet\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\shwet\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shwet\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61636df0-6e2e-4f48-a586-1437746959a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy without scaling: 0.8055555555555556\n",
      "KNN Accuracy with scaling : 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "#import the library \n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#laod the dataset\n",
    "\n",
    "wine = load_wine() \n",
    "x , y = wine.data, wine.target\n",
    "\n",
    "#train test split\n",
    "x_train , x_test, y_train, y_test = train_test_split(\n",
    "    x , y, test_size = 0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "#1. KNN without scaling \n",
    "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_no_scale.fit(x_train, y_train)\n",
    "y_pred_no_scale = knn_no_scale.predict(x_test)\n",
    "acc_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
    "\n",
    "#2 KNN with scaling\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(x_train_scaled, y_train)\n",
    "y_pred_scaled = knn_scaled.predict(x_test_scaled)\n",
    "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "#print results\n",
    "\n",
    "print('KNN Accuracy without scaling:', acc_no_scale)\n",
    "print('KNN Accuracy with scaling :', acc_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232eade-a17c-40e2-b0b4-de3ee230552f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76be25c6-b7cd-4fd7-8f39-72e29d332946",
   "metadata": {},
   "source": [
    "# Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
    " ratio of each principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8c2c0e7-9349-45ee-b331-92ddacd65f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio of each Principal Compponent:\n",
      "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
      " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
      " 0.00795215]\n",
      "\n",
      "Cumulative Expalined Variance:\n",
      "[0.36198848 0.55406338 0.66529969 0.73598999 0.80162293 0.85098116\n",
      " 0.89336795 0.92017544 0.94239698 0.96169717 0.97906553 0.99204785\n",
      " 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#load the wine set\n",
    "wine = load_wine()\n",
    "x, y = wine.data, wine.target\n",
    "\n",
    "#standardize the features (importance before pca)\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "#apply PCA\n",
    "pca = PCA()\n",
    "x_pca = pca.fit_transform(x_scaled)\n",
    "\n",
    "#print explained variance ratio\n",
    "print(\"Explained Variance Ratio of each Principal Compponent:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "#cumulative explained variance\n",
    "import numpy as np\n",
    "cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(\"\\nCumulative Expalined Variance:\")\n",
    "print(cum_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a313e8a-f266-4faa-b5db-474d0388a0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2004f1a-eda7-4812-babb-2a2b3d10c100",
   "metadata": {},
   "source": [
    "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
    "components). Compare the accuracy with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9da4f80c-bbf9-4782-978f-496ffd0b6f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy on Original (Scaled) datstets: 0.9722222222222222\n",
      "KNN Accuracy on PCA-reduced (2 componets) datasets : 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#load dataset\n",
    "wine = load_wine()\n",
    "x , y = wine.data, wine.target\n",
    "\n",
    "#train-test split\n",
    "x_train , x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size = 0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "knn_orig = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_orig.fit(x_train_scaled, y_train)\n",
    "y_pred_origin = knn_orig.predict(x_test_scaled)\n",
    "acc_orig = accuracy_score(y_test, y_pred_origin)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "x_train_pca = pca.fit_transform(x_train_scaled)\n",
    "x_test_pca = pca.transform(x_test_scaled)\n",
    "\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(x_train_pca, y_train)\n",
    "y_pred_pca = knn_pca.predict(x_test_pca)\n",
    "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
    "\n",
    "#print results\n",
    "\n",
    "print(\"KNN Accuracy on Original (Scaled) datstets:\" , acc_orig)\n",
    "print(\"KNN Accuracy on PCA-reduced (2 componets) datasets :\" , acc_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db32fe7a-73df-42f5-8001-7fce5f0665ba",
   "metadata": {},
   "source": [
    "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
    "manhattan) on the scaled Wine dataset and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38f100-1669-439e-bf8f-c68122dceab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features (important for distance-based methods)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ------------------------------\n",
    "# KNN with Euclidean Distance (p=2)\n",
    "# ------------------------------\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "knn_euclidean.fit(X_train_scaled, y_train)\n",
    "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
    "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
    "\n",
    "# ------------------------------\n",
    "# KNN with Manhattan Distance (p=1)\n",
    "# ------------------------------\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=1)\n",
    "knn_manhattan.fit(X_train_scaled, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
    "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
    "\n",
    "# Print results\n",
    "print(\"KNN Accuracy with Euclidean distance:\", acc_euclidean)\n",
    "print(\"KNN Accuracy with Manhattan distance:\", acc_manhattan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04000bbf-68fc-4043-89c8-6f21c6c9070d",
   "metadata": {},
   "source": [
    "Question 10: You are working with a high-dimensional gene expression dataset to\n",
    "classify patients with different types of cancer.\n",
    "Due to the large number of features and a small number of samples, traditional models\n",
    "overfit.\n",
    "Explain how you would:\n",
    "● Use PCA to reduce dimensionality\n",
    "● Decide how many components to keep\n",
    "● Use KNN for classification post-dimensionality reduction\n",
    "● Evaluate the model\n",
    "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
    "biomedical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b93dba-2cf5-4bda-a976-37602f65f762",
   "metadata": {},
   "source": [
    "# Question 10: PCA + KNN on High-Dimensional Gene Expression Dataset\n",
    "\n",
    "We have a dataset with many more **features (genes)** than **samples (patients)**.  \n",
    "This causes overfitting for traditional models.  \n",
    "\n",
    "We will:\n",
    "1. Use **PCA** to reduce dimensionality.  \n",
    "2. Tune the **number of principal components**.  \n",
    "3. Apply **KNN classification**.  \n",
    "4. Evaluate performance with **nested cross-validation**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2293662d-07dc-43cb-98d6-aca39b663aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c194710-95dd-4177-8996-52a67586d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine   # placeholder, replace with your gene data\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# (For plotting results later if needed)\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e309fb0-13c2-4a65-9955-b697459d1b0d",
   "metadata": {},
   "source": [
    "### Step 2: Load dataset\n",
    "\n",
    "Here we use the **Wine dataset** (13 features, 178 samples) as a placeholder.  \n",
    "In your case, replace this with your **gene expression matrix**:\n",
    "- `X` → rows = patients, columns = genes  \n",
    "- `y` → labels (cancer type)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c59cb-cc67-4d0e-96c3-6b0e4a1c8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wine dataset (replace this with your gene expression data)\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "print(\"Shape of data:\", X.shape)   # (samples, features)\n",
    "print(\"Number of classes:\", len(np.unique(y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff92fc4-e216-4077-ae8d-ad3dee454daf",
   "metadata": {},
   "source": [
    "### Step 3: Build a pipeline\n",
    "\n",
    "Pipeline:\n",
    "- StandardScaler → PCA → KNN  \n",
    "\n",
    "We will tune:\n",
    "- Number of PCA components  \n",
    "- Number of neighbors in KNN  \n",
    "- Distance metric (Euclidean vs Manhattan)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cef9d1-2111-41e8-b82c-fc9634c2f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(svd_solver=\"full\", random_state=42)),\n",
    "    (\"knn\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"pca__n_components\": [5, 10, 20, 30, 50, 75, 100],\n",
    "    \"knn__n_neighbors\": [3, 5, 7, 9, 11, 15],\n",
    "    \"knn__metric\": [\"minkowski\"],\n",
    "    \"knn__p\": [1, 2],   # 1 = Manhattan, 2 = Euclidean\n",
    "    \"knn__weights\": [\"uniform\", \"distance\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21864979-335f-4bb2-893c-1a7964ab2522",
   "metadata": {},
   "source": [
    "### Step 4: Nested Cross-Validation\n",
    "\n",
    "- **Inner loop (GridSearchCV):** finds the best parameters.  \n",
    "- **Outer loop:** gives unbiased performance estimate.  \n",
    "- We use **Macro-F1 score** (fair to all classes).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b37043-15ec-461c-b098-84a5ae1844cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad981a9-0725-43af-89c5-4bf1190096fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner CV for hyperparameter tuning\n",
    "inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Scorer: Macro-F1 (good for imbalanced classes)\n",
    "scorer = make_scorer(f1_score, average=\"macro\")\n",
    "\n",
    "# GridSearch with inner CV\n",
    "gs = GridSearchCV(pipeline, param_grid, scoring=scorer, cv=inner, n_jobs=-1)\n",
    "\n",
    "# Outer CV for unbiased evaluation\n",
    "outer = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# Run nested CV\n",
    "scores = cross_val_score(gs, X, y, scoring=scorer, cv=outer, n_jobs=-1)\n",
    "\n",
    "print(f\"Macro-F1 (Nested CV): {np.mean(scores):.3f} ± {np.std(scores):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b82a3d6-0219-4476-8908-6f3b1a81d607",
   "metadata": {},
   "source": [
    "### Step 5: Justification for stakeholders\n",
    "\n",
    "- **Dimensionality reduction:** PCA compresses thousands of gene features into a small set of informative signals.  \n",
    "- **Avoids overfitting:** Nested CV ensures robust performance.  \n",
    "- **KNN simplicity:** Non-parametric, interpretable, and easy to update.  \n",
    "- **Clinical trust:** PCs can be mapped back to genes → pathway analysis → biological interpretability.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eadaf3-ff1e-4195-9360-a352dd30fbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc94d407-1d03-4c0a-80ae-32376a0f2847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4113027-dea1-4cb0-95fd-bab82e1543b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
